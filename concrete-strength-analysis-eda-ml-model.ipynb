{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport sklearn.metrics as skm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-18T02:51:14.188232Z","iopub.execute_input":"2022-06-18T02:51:14.189153Z","iopub.status.idle":"2022-06-18T02:51:15.999306Z","shell.execute_reply.started":"2022-06-18T02:51:14.189016Z","shell.execute_reply":"2022-06-18T02:51:15.996911Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A. Preface\n\nIn this project, we will analyse a concrete data set which consists of\nmaterial compositions as features and the concrete strength data.\nFirst we will analyse the relation of the features as well as the\nrealation between each feature with the strength.\nThen we will build a machine learning model to predict the strength\nof given concrete compositions.","metadata":{}},{"cell_type":"markdown","source":"# B. Data Set Up","metadata":{}},{"cell_type":"code","source":"data_path = \"../input/concrete-compressive-strength-uci/Concrete_Data.csv\"\nconcrete_df = pd.read_csv(data_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:51:57.671967Z","iopub.execute_input":"2022-06-18T02:51:57.672505Z","iopub.status.idle":"2022-06-18T02:51:57.694918Z","shell.execute_reply.started":"2022-06-18T02:51:57.672464Z","shell.execute_reply":"2022-06-18T02:51:57.69391Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Overview\nprint(\"The first 10 rows of the data:\")\nconcrete_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:53:44.65595Z","iopub.execute_input":"2022-06-18T02:53:44.656407Z","iopub.status.idle":"2022-06-18T02:53:44.687253Z","shell.execute_reply.started":"2022-06-18T02:53:44.656373Z","shell.execute_reply":"2022-06-18T02:53:44.686393Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The shape of the data\nprint(f\"The number of rows of the data = {concrete_df.shape[0]}\")\nprint(f\"The number of columns of the data = {concrete_df.shape[1]}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:54:20.923715Z","iopub.execute_input":"2022-06-18T02:54:20.924235Z","iopub.status.idle":"2022-06-18T02:54:20.930145Z","shell.execute_reply.started":"2022-06-18T02:54:20.924198Z","shell.execute_reply":"2022-06-18T02:54:20.929311Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since the name of the columns are complicated in the original data set as we see\n# above, we will change the name of the columns for convenience.\nnew_columns = [\"cement\",\n               \"blast_furnace_slag\",\n               \"fly_ash\",\n               \"water\",\n               \"superplasticizer\",\n               \"coarse_aggregate\",\n               \"fine_aggregate\",\n               \"age\",\n               \"concrete_strength\"]\nconc_df = concrete_df.copy()\nconc_df.columns = new_columns\n\nprint(\"The first 10 rows of the data with new columns names:\")\nconc_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:54:44.03395Z","iopub.execute_input":"2022-06-18T02:54:44.034433Z","iopub.status.idle":"2022-06-18T02:54:44.058219Z","shell.execute_reply.started":"2022-06-18T02:54:44.034387Z","shell.execute_reply":"2022-06-18T02:54:44.057203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# C. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## C1. General Information","metadata":{}},{"cell_type":"code","source":"# Checking the existence of missing values\nprint(\"Existence of missing values on each columns:\")\nconc_df.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:58:02.199719Z","iopub.execute_input":"2022-06-18T02:58:02.200296Z","iopub.status.idle":"2022-06-18T02:58:02.211619Z","shell.execute_reply.started":"2022-06-18T02:58:02.200252Z","shell.execute_reply":"2022-06-18T02:58:02.210714Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General information about the data\nconc_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:58:14.998837Z","iopub.execute_input":"2022-06-18T02:58:14.999263Z","iopub.status.idle":"2022-06-18T02:58:15.02501Z","shell.execute_reply.started":"2022-06-18T02:58:14.99923Z","shell.execute_reply":"2022-06-18T02:58:15.023878Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistics Descriptive\n\n# Since all the columns are of numeric data type,\n# we do not need to perform numeric transformation\n# to the dataframe.\n# The statsitics descriptive of the data is given\n# as follows:\nprint(\"Statistics descpritive of the data:\")\nconc_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-06-18T02:58:27.07266Z","iopub.execute_input":"2022-06-18T02:58:27.073671Z","iopub.status.idle":"2022-06-18T02:58:27.123719Z","shell.execute_reply.started":"2022-06-18T02:58:27.07362Z","shell.execute_reply":"2022-06-18T02:58:27.122624Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## C2. Correlation\n\nNow we will analyse the correlations in the data set. **Correlation** can be\nintuitively thought of as the degree to which a pair of variables linearly\nrelated. The formal mathematical description of correlation is given as folllows.\n\nLet $(\\Omega,\\, \\mathcal{F},\\, P)$ be the probability space [1] corresponding\nthe data set. Let $\\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P)$ be the set of all\nrandom variables on $(\\Omega,\\, \\mathcal{F},\\, P)$. Then we have\n$\\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P) \\subseteq \\mathcal{L}^1(\\Omega,\\, \\mathcal{F},\\, P)$\n[3]. *Expected value* [2] is a map $\\mathrm{E}: \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P) \\to \\mathbb{R}$\ndefined by the Lebesgue integral [3]\n$$\n    \\forall X \\in \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P) \\,,\\;\n    \\mathrm{E}[X] := \\int_\\Omega X \\,dP \\,.\n$$\n*Covariance* [2] is a map\n$\\mathrm{cov}: \\mathcal{R}(\\Omega\\,, \\mathcal{F},\\, P) \\times \\mathcal{R}(\\Omega\\,, \\mathcal{F},\\, P)\n\\to \\mathbb{R}$ defined by\n$$\n    \\forall X, Y \\in \\mathcal{R}(\\Omega\\,, \\mathcal{F},\\, P) \\,,\\;\n    \\mathrm{cov}(X, Y) := \\mathrm{E}[ (X - \\mathrm{E}[X]) \\, (Y - \\mathrm{E}[Y]) \\,.\n$$\nNow let $U, V \\in \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P)$. Let $A, B, C \\in \\mathbb{R}$\nsuch that\n$$\n    \\mathrm{E}[U] = \\int_\\Omega U \\,dP := A \\,,\n$$\n$$\n    \\mathrm{E}[V] = \\int_\\Omega V \\,dP := B \\,,\n$$\nand\n$$\n    \\mathrm{E}[UV] = \\int_\\Omega UV \\,dP := C \\,.\n$$\nThen from the definition of covariance and by the property of Lebesgue integrable functions [3],\nwe obtain\n\n$$\n\\begin{split}\n    \\mathrm{cov}(U, V) &= \\mathrm{E}\\big[ (U - \\mathrm{E}[U]) \\, (V - \\mathrm{E}[V]) \\big]\\\\\n                       &= \\mathrm{E}\\big[ UV - U\\mathrm{E}[V] - \\mathrm{E}[U]V\n                           + \\mathrm{E}[U]\\mathrm{E}[V] \\big] \\\\\n                       &= \\int_\\Omega \\left( UV - UB - AV + AB \\right) \\,dP \\\\\n                       &= \\int_\\Omega UV \\,dP - \\int_\\Omega UB \\,dP - \\int_\\Omega AV \\,dP\n                           + \\int_\\Omega AB \\,dP \\\\\n                       &= \\int_\\Omega UV \\,dP - B \\int_\\Omega U \\,dP - A\\int_\\Omega V \\,dP\n                           + AB \\int_\\Omega \\,dP \\\\\n                       &= C - AB - AB + AB \\cdot P(\\Omega) \\\\\n                       &= C - 2 AB + AB \\\\\n                       &= C - AB \\\\\n                       &= \\mathrm{E}[UV] - \\mathrm{E}[U]\\mathrm{E}[V] \\,.\n\\end{split}\n$$\n\n**Pearson correlation** is a map\n$\\mathrm{corr}: \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P) \\times \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P)\n\\to \\mathbb{R}$ defined by\n$$\n    \\forall X, Y \\in \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P) \\,,\\;\n    \\mathrm{corr}(X, Y) := \\frac{\\mathrm{cov}(X, Y)}{\\sqrt{\\mathrm{cov}(X, X) \\, \\mathrm{cov}(Y, Y)}}\n    \\,,\n$$\nwhich can be expanded into\n\n$$\n\\begin{split}\n    \\mathrm{corr}(X, Y)\n    &= \\frac{\\mathrm{cov}(X, Y)}{\\sqrt{\\mathrm{cov}(X, X) \\, \\mathrm{cov}(Y, Y)}} \\\\\n    &= \\frac{ \\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y] }{\n        \\sqrt{\\big( \\mathrm{E}[X^2] - \\mathrm{E}[X]^2 \\big)\n        \\big( \\mathrm{E}[Y^2] - \\mathrm{E}[Y]^2 \\big)} } \\\\\n    &= \\frac{ \\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y] }{\n        \\sqrt{ \\mathrm{E}[X^2] \\mathrm{E}[Y^2] - \\mathrm{E}[X^2] \\mathrm{E}[Y]^2\n        - \\mathrm{E}[X]^2 \\mathrm{E}[Y^2] + \\mathrm{E}[X]^2 \\mathrm{E}[Y]^2 }\n        } \\,.\n\\end{split}\n$$\n\nfor every $X, Y \\in \\mathcal{R}(\\Omega,\\, \\mathcal{F},\\, P)$.\n\nFor this project, we will make use of `pandas.DataFframe.corr` for correlation.\nBy default, the method uses Pearson correlation.\n\n**References**\n\n[1] Kolmogorov, A. N. (1950). \"Foundations of the Probability Theory\". New York: Chelsea.\n\n[2] Movellan, Javier R. (2008). \"Introduction to Probability Theory and Statistics\". Author.\n\n[3] Salamon, Dietmar A. (2016). \"Measure and Integration\". European Mathematical Society.","metadata":{}},{"cell_type":"markdown","source":"The correlation table of the data is given as follows.","metadata":{}},{"cell_type":"code","source":"# Correlation between features\nfeatures_corr = conc_df.corr()\nfeatures_corr","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:01:33.800817Z","iopub.execute_input":"2022-06-18T03:01:33.801361Z","iopub.status.idle":"2022-06-18T03:01:33.823514Z","shell.execute_reply.started":"2022-06-18T03:01:33.801319Z","shell.execute_reply":"2022-06-18T03:01:33.822493Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To have a better intuitive insights from the correlation table,\nlet us observe the visualisation as follows.","metadata":{}},{"cell_type":"code","source":"# Visualisation of the Correlations\nsns.set_style(\"dark\")\ncorr_fig = plt.figure(figsize= (2* 10, 1* 6))\ncorr_gs = corr_fig.add_gridspec(1, 2)\ncorr_ax = [[corr_fig.add_subplot(corr_gs[i, j])\n            for j in range(2)] for i in range(1)]\n\n# Heatmap representation of the correlation\n# Heatmap\ncorr_00 = sns.heatmap(features_corr,\n                      annot= True,\n                      linewidth= 0.1,\n                      cmap= \"Blues\",\n                      ax= corr_ax[0][0])\nhm_title_01 = \"HEATMAP OF PEARSON CORRELATIONS BETWEEN FEATURES\"\nhm_title_02 = \"Heatmap of Pearson Correlations Between Features\"\ncorr_00.set_title(hm_title_02, pad= 12)\n\n# Barplot\n# The correlation of each features and concrete strength\ncfcs_cod = features_corr[\"concrete_strength\"][0:-1]\ncfcs_dom = cfcs_cod.index\n\ncorr_01 = sns.barplot(x= cfcs_cod, y= cfcs_dom, palette= \"cool\",\n                      ax= corr_ax[0][1])\ncorr_01.set_title(\"Correlation of Each Feature with Concrete Strength\",\n                  pad= 12)\ncorr_01.set(xlabel= \"Correlation with Concrete Strength\",\n            ylabel= \"Features\",\n            xlim= (-cfcs_cod.max() - 0.15, cfcs_cod.max() + 0.15)\n           )\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.8, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:01:51.20907Z","iopub.execute_input":"2022-06-18T03:01:51.210082Z","iopub.status.idle":"2022-06-18T03:01:52.078034Z","shell.execute_reply.started":"2022-06-18T03:01:51.210033Z","shell.execute_reply":"2022-06-18T03:01:52.077263Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visualisation, we can see that the amount of cements positively correlated to\nthe concrete strength. In fact, it has the highest positive correlation with the\nstrength compared to the other features'. Superplasticizers also increase the strength,\nand so does the age of the concrete. On the other hand, water, course aggregates, fine\naggregates and fly ash have negative correlations to the stength. This is understandable,\nsince as we increase the number of these materials within a concrete mixture, we must\ndecrease the number of cements. This is the consequence of the density equilibrium.\n\nAs we look closer, we also find that `water` and `superplasticizer` are negatively\ncorrelated. In fact, the reason why engineers apply superplactiziers to a concrete\nmixture is to increase the workability so that it allows for less water composition,\nwhich again means increasing concrete strength (as we see that water is negatively\ncorrelated with strength). It describes why `superplasticizer` and `concrete_strength`\nare positively correlated.","metadata":{}},{"cell_type":"markdown","source":"## C3. Further Relations\nLet us observe further relations between the features in the data.\nNow that we know `cement`, `superplasticizer` and `age` have positive correlations with\n`concrete_strength`, we will try to plot the relation between each of `cement`, `superplasticizer`\nand `age` with `concrete_strength`.","metadata":{}},{"cell_type":"code","source":"class FurtherRelations:\n    def __init__(self, df, strength):\n        self.df = df\n        self.strength = strength\n        self.all_df = None\n        \n    def __sp_labeller(self):\n        ndf = self.df\n        sp_cat = []\n        for sp in ndf[\"superplasticizer\"]:\n            low_b = 3* int(sp / 3)\n            upp_b = low_b + 3\n            sp_cat.append(f\"[{str(low_b), {str(upp_b)}})\")\n        ndf[\"SP_cat\"] = sp_cat\n        self.df = ndf\n        \n    def weight(self):\n        ndf = self.df\n        df_idx_w = list(ndf.columns)[0: -1]\n        ndf[\"weight\"] = [ndf.loc[k, df_idx_w].sum()\n                             for k in range(len(ndf))]\n        self.df = ndf\n        \n    def generate_all(self):\n        ndf = self.df.copy()\n        ndf[\"concrete_strength\"] = self.strength\n        self.all_df = ndf\n        \n    def hist_cement_sp_age_all(self):\n        sns.set_style(\"dark\")\n        fig = plt.figure(figsize= (3 *8, 6))\n        gs = fig.add_gridspec(1, 3)\n        ax = [[fig.add_subplot(gs[i, j]) for j in range(3)]\n              for i in range(1)]\n        \n        cols = [\"cement\", \"superplasticizer\", \"age\"]\n        col_titles = [\"Cement Compositions\",\n                      \"SP Compositions\",\n                      \"Concrete Age\"]\n        ax_00, ax_01, ax_02 = None, None, None\n        bin_widths = [25, 3, 15]\n        axs = [ax_00, ax_01, ax_02]\n        for k, col, col_title, ax_, bw in zip(range(3),\n                                             cols,\n                                             col_titles,\n                                             axs,\n                                             bin_widths):\n            ax_ = sns.histplot(data= self.df[col],\n                               binwidth= bw,\n                               stat= \"density\",\n                               kde= True,\n                               ax= ax[0][k])\n            ax_.set_title(f\"Distribution of the {col_title}\", pad= 15)\n            ax_.set(xlabel= col_title + \" (kg/m^3)\",\n                    ylabel= \"Density\")\n        plt.show()\n        \n    def cement_dist_normal(self):\n        used_df = self.df[(self.df[\"superplasticizer\"] == 0)\n                          & ((self.df[\"age\"] == 28)\n                             | (self.df[\"age\"] ==  14))][\"cement\"]\n        \n        sns.set_style(\"dark\")\n        fig = plt.figure(figsize= (8, 6))\n        fig = sns.histplot(data= used_df, binwidth= 25)\n        fig.set_title(f\"Dist. of Cement Comp. without Superplasticizer on Day 14 and 28 ({len(used_df)} Samples)\",\n                      pad= 12)\n        fig.set(xlabel= \"Cement Compositions (kg/m^3)\", ylabel= \"Count\")\n        plt.show()\n        \n    def dist_cement_mature(self):\n        self.generate_all()\n        df = self.all_df\n        used_data = [\n            df[(df[\"age\"] == 28) & (df[\"superplasticizer\"] < 10)],\n            df[(df[\"age\"] == 28) & (10 <= df[\"superplasticizer\"])\n               & (df[\"superplasticizer\"] < 20)],\n            df[(df[\"age\"] == 28) & (df[\"superplasticizer\"] >= 20)]\n        ]\n        titles = [\"SP < 10\", \"10 ≤ SP < 20\", \"SP ≥ 20\"]\n        \n        sns.set_style(\"dark\")\n        fig = plt.figure(figsize= (3 *8, 2 *6))\n        gs = fig.add_gridspec(2, 3)\n        ax = [[fig.add_subplot(gs[i, j]) for j in range(3)] for i in range(2)]\n        ax_00, ax_01, ax_02, ax_10, ax_11, ax_12  = None, None, None, None, None, None\n        \n        axs_0 = [ax_00, ax_01, ax_02]\n        for k, dat, ax_, tit in zip(range(3), used_data, axs_0, titles):\n            ax_ = sns.histplot(data= dat[\"cement\"],\n                               stat= \"density\",\n                               kde= True,\n                               ax= ax[0][k])\n            ax_.set_title(f\"Dist. Cement Comp. with {tit} on Day 28\", pad= 12)\n            ax_.set(xlabel= \"Cement Composition (kg/m^3)\",\n                    ylabel= \"Density\")\n        \n        axs_1 = [ax_10, ax_11, ax_12]\n        for k, dat, ax_, tit in zip(range(3), used_data, axs_1, titles):\n            ax_ = sns.regplot(x= dat[\"cement\"],\n                                  y= dat[\"concrete_strength\"],\n                                  ax= ax[1][k])\n            ax_.set_title(f\"Relation Between Cement Comp. & Strength ({tit})\", pad= 12)\n            ax_.set(xlabel= \"Cement Composition (kg/m^3)\",\n                    ylabel= \"Compressive Strength (MPa)\")\n            \n        plt.subplots_adjust(hspace= 0.25)\n        plt.show()\n    \n    def cement_strengh_relation(self):\n        df = self.df.copy()\n        sp_cat = []\n        for sp in df[\"superplasticizer\"]:\n            if sp < 10:\n                sp_cat.append(f\"[0, 10)\")\n            elif 10 <= sp < 20:\n                sp_cat.append(f\"[10, 20)\")\n            else:\n                sp_cat.append(f\"[20, max sp]\")\n        df[\"sp_cat\"] = sp_cat\n        df[\"concrete_strength\"] = self.strength\n        ndf = df[(df[\"age\"] == 28)]\n        \n        sns.set_style(\"dark\")\n        fig = plt.figure(figsize= (8, 6))\n        fig = sns.regplot(x= \"cement\",\n                          y= \"concrete_strength\",\n                          data= ndf)\n        fig.set_title(\"The Relation bertween Cement and Concrete Strength on Day 28\",\n                      pad= 12)\n        fig.set(xlabel= \"Cement Compositions (kg/m^3)\",\n                ylabel= \"Concrete Compressive Strength (MPa)\")\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:10:55.995295Z","iopub.execute_input":"2022-06-18T03:10:55.996041Z","iopub.status.idle":"2022-06-18T03:10:56.400695Z","shell.execute_reply.started":"2022-06-18T03:10:55.995938Z","shell.execute_reply":"2022-06-18T03:10:56.399602Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualisations of the Relations\nft_df = conc_df.copy()\nc_str = ft_df.concrete_strength\nft_df.drop(columns= [\"concrete_strength\"], inplace= True)\n\nft_relations = FurtherRelations(ft_df, c_str)\nft_relations.hist_cement_sp_age_all()\nft_relations.dist_cement_mature()\nft_relations.cement_strengh_relation()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:11:13.096033Z","iopub.execute_input":"2022-06-18T03:11:13.09691Z","iopub.status.idle":"2022-06-18T03:11:15.440522Z","shell.execute_reply.started":"2022-06-18T03:11:13.096862Z","shell.execute_reply":"2022-06-18T03:11:15.439447Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# D. Machine Learning Model\n\nNow we will build the machine learning (ML) model based on the knowledge\nwe know so far. The possible algorithms that we can use are\n`LinearRegression`, `KNeighborsRegressor`, `RandomForestRegressor`,\nand `XGBRegressor`. Then we will observe which one is the best model by\nevaluating their mean absolute error (MAE) score.","metadata":{}},{"cell_type":"markdown","source":"## D1. Preprocessing\n\nNormalisation probably improves the model performance, or probably\ndegrades either. However, we will evaluate the performance of the\nmodel with or without normalisation. The normalisation that\nwe use is the `MinMaxScaler` from `sklearn.preprocessing`.","metadata":{}},{"cell_type":"markdown","source":"## D2. Learning Model","metadata":{}},{"cell_type":"code","source":"class ModelScorer:\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __data_criterion(self):\n        if isinstance(self.X, pd.core.frame.DataFrame) \\\n                and isinstance(self.y, pd.core.series.Series):\n            return True\n        else:\n            return False\n    \n    def cv_single_score(self, model, normalise= False):\n        from statistics import mean\n        if self.__data_criterion():\n            X, y = self.X, self.y\n            if normalise == False:\n                my_pipeline = Pipeline(\n                    steps= [(\"model\", model)]\n                )\n            else:\n                my_pipeline = Pipeline(\n                    steps= [(\"preprocessing\", MinMaxScaler()),\n                            (\"model\", model)]\n                )\n            mae_scores = -1 * cross_val_score(my_pipeline,\n                                              X, y,\n                                              cv= 5,\n                                              scoring= \"neg_mean_absolute_error\")\n            return mean(mae_scores)\n    \n    def cv_collective_score(self, models, model_index):\n        \"\"\">>> 'models' and 'model_index' must have the same length.\"\"\"\n        X = self.X\n        y = self.y\n        scorer = self.cv_single_score\n        cv_sc_dic = {\"cv_model_no_scale\": [scorer(model)\n                                           for model in models],\n                     \"cv_model_scaled\": [scorer(model, normalise= True)\n                                         for model in models]}\n        cv_sc_df = pd.DataFrame(cv_sc_dic, index= model_index)\n        return cv_sc_df","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:14:18.168729Z","iopub.execute_input":"2022-06-18T03:14:18.169321Z","iopub.status.idle":"2022-06-18T03:14:18.182142Z","shell.execute_reply.started":"2022-06-18T03:14:18.169283Z","shell.execute_reply":"2022-06-18T03:14:18.180854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation\nmodels = [LinearRegression(n_jobs= 6),\n          KNeighborsRegressor(p= 1, n_jobs= 6),\n          KNeighborsRegressor(p= 2, n_jobs= 6),\n          RandomForestRegressor(n_estimators= 500, n_jobs= 6),\n          XGBRegressor(n_estimators= 500, learning_rate= 0.01, n_jobs= 6)]\nmodel_index = [\"Linear_Regression\",\n               \"KN_Regression_Taxicab\",\n               \"KN_Regression_Euclidean\",\n               \"Random_Forest_Regression\",\n               \"XGB_Regression\"]\n\nX = conc_df.drop(columns= [\"concrete_strength\"])\ny = conc_df.concrete_strength\nmodel_eval = ModelScorer(X, y)\nmodel_cv_sc_df = model_eval.cv_collective_score(models, model_index)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:15:58.443753Z","iopub.execute_input":"2022-06-18T03:15:58.444323Z","iopub.status.idle":"2022-06-18T03:16:40.182215Z","shell.execute_reply.started":"2022-06-18T03:15:58.444279Z","shell.execute_reply":"2022-06-18T03:16:40.180952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Score DF\nprint(\"Model score based on the evaluation:\")\nmodel_cv_sc_df","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:16:40.184453Z","iopub.execute_input":"2022-06-18T03:16:40.185288Z","iopub.status.idle":"2022-06-18T03:16:40.197366Z","shell.execute_reply.started":"2022-06-18T03:16:40.185232Z","shell.execute_reply":"2022-06-18T03:16:40.19637Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Score Visualisation\nsns.set_style(\"dark\")\nms_fig = plt.figure(figsize= (8, 6))\nms_fig = sns.lineplot(data= model_cv_sc_df, marker= \"o\")\nms_fig.set_title(\"Model Score\", pad= 12)\nms_fig.set(xlabel= \"Models\",\n           ylabel= \"Cross Validation Score: MAE\",\n           ylim= (7, 11))\nplt.xticks(rotation= 45)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T03:21:14.823738Z","iopub.execute_input":"2022-06-18T03:21:14.824271Z","iopub.status.idle":"2022-06-18T03:21:15.035315Z","shell.execute_reply.started":"2022-06-18T03:21:14.824233Z","shell.execute_reply":"2022-06-18T03:21:15.034297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## D3. Model Optimisastion\nNote that the best model gives the least MAE score.\nFrom the model evaluation we can see that `XGB_Regression` outplays the other\nmodels. However, the absolute error is still around 7.68. We wish to\ndecrease the absolute error even further. To achieve that, we will try\nto seek the best `n_estimators` for the model.","metadata":{}},{"cell_type":"code","source":"pos_nest = [k for k in range(100, 1000, 100)]\nmod_opt_dic = {\"XGBR_no_scale\": [model_eval.cv_single_score(\n                   XGBRegressor(n_estimators= k,\n                                learning_rate= 0.01,\n                                n_jobs= 6))\n                    for k in pos_nest],\n               \"XGBR_scaled\": [model_eval.cv_single_score(\n                   XGBRegressor(n_estimators= k,\n                                learning_rate= 0.01,\n                                n_jobs= 6),\n                   normalise= True)\n                    for k in pos_nest]}\nmod_opt_df = pd.DataFrame(mod_opt_dic, index= pos_nest)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:23:00.782941Z","iopub.execute_input":"2022-06-18T03:23:00.78342Z","iopub.status.idle":"2022-06-18T03:27:09.102235Z","shell.execute_reply.started":"2022-06-18T03:23:00.783388Z","shell.execute_reply":"2022-06-18T03:27:09.100923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The Model MAE score based on the number of estimators:\")\nmod_opt_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T03:27:09.104223Z","iopub.execute_input":"2022-06-18T03:27:09.104924Z","iopub.status.idle":"2022-06-18T03:27:09.116649Z","shell.execute_reply.started":"2022-06-18T03:27:09.104876Z","shell.execute_reply":"2022-06-18T03:27:09.115683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model MAE Score Visualisation\nsns.set_style(\"dark\")\nxgb_fig = plt.figure(figsize= (2 *8, 6))\nxgb_gs = xgb_fig.add_gridspec(1, 2)\nxgb_ax = [[xgb_fig.add_subplot(xgb_gs[0, j]) for j in range(2)]]\n\nxgb_ax_00 = sns.lineplot(data= mod_opt_df[\"XGBR_no_scale\"],\n                         marker= \"o\",\n                         ax= xgb_ax[0][0])\nxgb_ax_00.set_title(\"MAE Score of the Model without Scaling\", pad= 12)\nxgb_ax_00.set(xlabel= \"Number of Estimators\",\n              ylabel= \"MAE Score\",\n              ylim= (6, 15))\n\nxgb_ax_01 = sns.lineplot(data= mod_opt_df[\"XGBR_scaled\"],\n                         marker= \"o\",\n                         color= \"orange\",\n                         ax= xgb_ax[0][1])\nxgb_ax_01.set_title(\"MAE Score of the Model with Scaling\", pad= 12)\nxgb_ax_01.set(xlabel= \"Number of Estimators\",\n              ylabel= \"MAE Score\",\n              ylim= (6, 15))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:48:13.066541Z","iopub.execute_input":"2022-06-18T03:48:13.067208Z","iopub.status.idle":"2022-06-18T03:48:13.446072Z","shell.execute_reply.started":"2022-06-18T03:48:13.067168Z","shell.execute_reply":"2022-06-18T03:48:13.445307Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result shows that as we increase the number of estimators the model gives a better performance.\nHowever, the improvement does not seem significant for the number of estimators greater than 300.\nTherefore, it is wiser to pick an effective number of estimators. Therefore we pick 600.\nOn the other hand, normalisation gives a slightly worse performance on `XGBRegressor`, therefore\nwe will omit normalisation for the final model.","metadata":{}},{"cell_type":"code","source":"final_model = XGBRegressor(n_estimators= 500, learning_rate= 0.01, n_jobs= 6)\nfinal_model.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:50:25.550652Z","iopub.execute_input":"2022-06-18T03:50:25.551249Z","iopub.status.idle":"2022-06-18T03:50:28.34607Z","shell.execute_reply.started":"2022-06-18T03:50:25.551206Z","shell.execute_reply":"2022-06-18T03:50:28.344978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual Data vs Prediction Comparison\nfinal_df = conc_df.copy()\ny_pred = final_model.predict(X)\nfinal_df[\"predicted_strength\"] = y_pred\nfinal_df[\"act_min_pred\"] = final_df[\"concrete_strength\"] - final_df[\"predicted_strength\"]\n\nprint(\"The data embedded with the predicted strength:\")\nfinal_df","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:52:11.184414Z","iopub.execute_input":"2022-06-18T03:52:11.18503Z","iopub.status.idle":"2022-06-18T03:52:11.237411Z","shell.execute_reply.started":"2022-06-18T03:52:11.184985Z","shell.execute_reply":"2022-06-18T03:52:11.23592Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The relations between actual and predicted strength are visualised as follows.","metadata":{}},{"cell_type":"code","source":"# Visualisation: Comparison between actual and predicted strength\ndef actual_prediction_comparison(df, act_col, pred_col, hue_cols,\n                                 viz1_tit= \"Actual Strength and Predicted Strength\",\n                                 viz1_xlabel= \"Actual Strength (MPa)\",\n                                 viz1_ylabel= \"Predicted Strengh (MPa)\",\n                                 viz2_tit= \"Difference Between Actual and Prediction\",\n                                 viz2_xlabel= \"Actual Strength (MPa)\",\n                                 viz2_ylabel= \"Difference (MPa)\"):\n    n_row, n_col = len(hue_cols), 2\n    fin_fig = plt.figure(figsize= (n_col* 8, n_row* 8))\n    fin_gs = fin_fig.add_gridspec(n_row, n_col)\n    fin_ax = [[fin_fig.add_subplot(fin_gs[i, j]) for j in range(n_col)]\n              for i in range(n_row)]\n    for nr, hc in zip(range(n_row), hue_cols):\n        # Scatter 1\n        finax_00 = sns.scatterplot(x= act_col,\n                                   y= pred_col,\n                                   hue= hc,\n                                   data= df,\n                                   palette= \"cool\",\n                                   ax= fin_ax[nr][0])\n        finax_00.set_title(viz1_tit, pad= 12)\n        finax_00.set(xlabel= viz1_xlabel,\n                     ylabel= viz1_ylabel)\n        # Scatter 2\n        finax_10 = sns.scatterplot(x= act_col,\n                                   y= df[act_col] - df[pred_col],\n                                   data= df,\n                                   hue= hc,\n                                   palette= \"cool\",\n                                   ax= fin_ax[nr][1])\n        finax_10.set_title(viz2_tit, pad= 12)\n        finax_10.set(xlabel= viz2_xlabel,\n                     ylabel= viz2_ylabel)\n        \n    plt.subplots_adjust(hspace= 0.3)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:53:18.486662Z","iopub.execute_input":"2022-06-18T03:53:18.487289Z","iopub.status.idle":"2022-06-18T03:53:18.500732Z","shell.execute_reply.started":"2022-06-18T03:53:18.487243Z","shell.execute_reply":"2022-06-18T03:53:18.499701Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hue_columns = [\"superplasticizer\", \"age\"]\nactual_prediction_comparison(final_df,\n                             \"concrete_strength\",\n                             \"predicted_strength\",\n                             hue_columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T03:54:21.355877Z","iopub.execute_input":"2022-06-18T03:54:21.356406Z","iopub.status.idle":"2022-06-18T03:54:23.018881Z","shell.execute_reply.started":"2022-06-18T03:54:21.356366Z","shell.execute_reply":"2022-06-18T03:54:23.017893Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}